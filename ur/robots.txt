# robots.txt for AI Textbook Platform

# Allow all search engines to crawl the site
User-agent: *
Allow: /

# Sitemap location (will be generated by Docusaurus)
Sitemap: https://YOUR-USERNAME.github.io/ai-book/sitemap.xml

# Disallow crawling of build artifacts and internal directories
Disallow: /build/
Disallow: /.docusaurus/
Disallow: /node_modules/

# Allow crawling of all documentation and content
Allow: /docs/
Allow: /blog/
Allow: /ur/

# Crawl delay (optional, adjust if needed)
# Crawl-delay: 1
