---
id: index
title: Chapter 11 - Vision-Language-Action (VLA) Models
sidebar_label: Overview
sidebar_position: 1
---

# Chapter 11: Vision-Language-Action (VLA) Models

## Overview

Explore Vision-Language-Action models that enable robots to understand natural language instructions and execute corresponding actions.

## What You'll Learn

- VLA model architecture
- Multimodal learning for robotics
- Language-conditioned policies
- Vision transformers for robotics
- Pre-trained VLA models (RT-1, RT-2, PaLM-E)
- Prompt engineering for robot control

## Prerequisites

- Completed Chapter 10: Isaac Locomotion
- Understanding of deep learning
- Familiarity with transformers and LLMs

## Estimated Time

5-6 hours

## Module

**Module 4: Vision-Language-Action** (Weeks 11-13)

---

**Status**: ðŸš§ Content in development

**Next Chapter**: [Chapter 12: VLA Training](../chapter-12-vla-training/)
